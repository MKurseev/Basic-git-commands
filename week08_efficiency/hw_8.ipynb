{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npIrOYmGWU7b"
      },
      "source": [
        "# Model Quantization\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2025/week08_efficiency/hw_8.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "In this session, you're going to implement post-training quantization approaches for _Large Language Models_, ranging from naive ones to State of The Art techniques. The main goal is to implement [GPTQ](https://arxiv.org/abs/2210.17323), with some of it's newer extension left as bonus exercises.\n",
        "\n",
        "<font color='red'>Important note:</font>\n",
        "This homework is designed to run on colab with T4 gpu. It requires at least 15Gb of *VRAM*, 12Gb of *RAM*. If your machine meets those criteria, you should be good to go too.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld5GgOmT2WE1"
      },
      "source": [
        "# Installing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install transformers==4.57.1 sentencepiece==0.2.1 datasets==4.0.0 accelerate==1.11.0 Ninja==1.13.0 triton==3.4.0"
      ],
      "metadata": {
        "id": "T4g6yVlMNOO-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtpmhiksi62J"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBpuR3vpi62J",
        "outputId": "77ab6c0e-d850-462a-cdb3-825e77d11381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
            "env: CUDA_VISIBLE_DEVICES=0 # Change it if you're on a multy-GPU machine\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
        "%env CUDA_VISIBLE_DEVICES=0 # Change it if you're on a multy-GPU machine\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import Mapping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaForCausalLM\n",
        "from transformers.models.llama.configuration_llama import LlamaConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncYOzQJeuQSY"
      },
      "source": [
        "**INT8 and INT4 GEMM custom CUDA kernels**\n",
        "\n",
        "Along with this jupyter notebook, two `CUDA` files are provided, implementing efficient matrix multiplication for `int4` and `int8`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/cutlass.git\n",
        "!export CUDACXX=/usr/local/cuda/bin/nvcc\n",
        "!cd cutlass && mkdir build && cd build && cmake .. -DCUTLASS_NVCC_ARCHS=75\n",
        "\n",
        "!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/kernel.cpp --no-check-certificate\n",
        "!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/kernel.cu --no-check-certificate"
      ],
      "metadata": {
        "id": "OmbaXHEDuTqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb1f5d7-10f3-410a-9f98-73eac3946c72"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cutlass' already exists and is not an empty directory.\n",
            "mkdir: cannot create directory ‘build’: File exists\n",
            "File ‘kernel.cpp’ already there; not retrieving.\n",
            "\n",
            "File ‘kernel.cu’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QF47Yk_zuQSa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "custom_kernel = load(name='custom_kernel', sources=['kernel.cpp', 'kernel.cu'], extra_include_paths=[r\"cutlass/include\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fEv4NL2uQSb"
      },
      "source": [
        "Usage: `output = custom_kernel.int8_matmul(X, Y)` and `output = custom_kernel.int4_matmul(X, Y)` compute $XY^T$ (same as `nn.functional.linear`).\n",
        "\n",
        "Notice that `int8_matmul` takes the normal `torch.int8` tensors, but int4_matmul expects `int4` values densely packed into `torch.uint8` tensors. This is exactly what we wrote **Dense Integer Packing** for. They have a <font color='red'>severe limitation</font>: the dimension of multiplication must be divisible by 16.\n",
        "\n",
        "Now you have to implement the quantized forward pass:\n",
        "\n",
        " $$\n",
        " \\begin{align}\n",
        "    XW^T &= (Q_x \\cdot scale_x + zero_x \\cdot scale_x)(Q_w \\cdot scale_w + zero_w \\cdot scale_w)^T =\\\\\n",
        "    &= (Q_x  + zero_x)(Q_w + zero_w)^T \\cdot (scale_x \\odot scale_w^T) =\\\\\n",
        "    &= (Q_xQ_w^T + Q_x zero_w^T + zero_x Q_w^T + zero_x zero_w^T) \\cdot (scale_x \\odot scale_w^T)\n",
        " \\end{align}\n",
        " $$\n",
        "\n",
        "because of the kernel limitations mentioned above, only the largest integer multiplication ($Q_xQ_w^T$) can be done in `int`. Perform the other ones in `float`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"NousResearch/Llama-3.2-1B\"\n",
        "\n",
        "def get_llama_model_and_tokenizer():\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"cpu\", dtype=torch.float16,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "yayC4hD7tjya"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPUs, Throughput and Quantization"
      ],
      "metadata": {
        "id": "GpK5PF-P9WNU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chFnB1GN-Jgu"
      },
      "source": [
        "Torch natively supports `int8`, but not `int4`. For the most part, we will `torch.uint8` to represent tensors quantized to *4 bits*. We encoded each *4-bit* value with an *8-bit* value, using twice as much memory as we really needed. We did this because `torch` lacks support for 4-bit tensors. However, we never performed any native operations in *8 bits*; we always converted it to `torch.float16` first. That means we can design a more efficient way to encode *4-bit* tensors and convert to and from it.\n",
        "\n",
        "Your task is to implement these conversions, converting *8-bit* tensors containing *4-bit* values (the way we used to store quantized weights) to and from smaller *8-bit* tensor, utilizing the whole range of values. Moreover, you'll have to do it in a way such that memory representation of contiguous arrays wouldn't change. That is, adjacent columns get squashed together.\n",
        "\n",
        "**Task (1.0pt):** Implement dense *int4*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MNprlCWn-Jgw"
      },
      "outputs": [],
      "source": [
        "def dense_pack_4_8(x: Tensor) -> Tensor:\n",
        "    \"\"\"Constructs a densely packed int tensor\n",
        "    Args:\n",
        "        x (Tensor): uint8 with uint4 values range\n",
        "\n",
        "    Returns:\n",
        "        Tensor: twice as small uint8 tensor with uint8 values range\n",
        "    \"\"\"\n",
        "    x = x.clone().detach()\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    pairs = x.reshape(*x.shape[:-1], -1, 2)\n",
        "    packed = pairs[..., 0] + pairs[..., 1] * 16\n",
        "\n",
        "    return packed.to(torch.uint8)\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "def dense_unpack_4_8(x: Tensor) -> Tensor:\n",
        "    \"\"\"Deconstructs a densely packed int tensor\n",
        "    Args:\n",
        "        x (Tensor): uint8 tensor with uint8 values range\n",
        "\n",
        "    Returns:\n",
        "        Tensor: twice as large int8 tensor with uint4 values range\n",
        "    \"\"\"\n",
        "    x = x.clone().detach()\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    unpacked = torch.stack([x % 16, x // 16], dim=-1)\n",
        "    unpacked = unpacked.reshape(*x.shape[:-1], -1)\n",
        "\n",
        "    return unpacked.to(torch.uint8)\n",
        "\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4AZQO84e-Jgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d517b2-a334-45b8-9e16-ccc4042e0199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing your code\n",
        "\n",
        "quantized_x = torch.randint(0, 16, (512 * 1024,)).reshape(512, 1024).float()\n",
        "dense_quantized_x = dense_pack_4_8(quantized_x)\n",
        "undense_quantized_x = dense_unpack_4_8(dense_quantized_x)\n",
        "\n",
        "assert dense_quantized_x.shape == (512, 512)\n",
        "assert torch.all(undense_quantized_x == quantized_x)\n",
        "assert torch.all(dense_unpack_4_8(dense_pack_4_8(quantized_x[:8,:8])) == dense_unpack_4_8(dense_pack_4_8(quantized_x)[:8,:4])), \"Adjacent values shall be packed\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can benchmark the quantized GEMM kernels to estimate the speedups:"
      ],
      "metadata": {
        "id": "VZ0liP9hDG1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from triton.testing import do_bench\n",
        "\n",
        "HIDDEN_SIZE = 4096\n",
        "NUM_REPEATS = 1024\n",
        "\n",
        "B_fp16 = torch.randint(0, 16, (HIDDEN_SIZE, HIDDEN_SIZE), dtype=torch.float16, device=\"cuda\")\n",
        "B_int8 = B_fp16.to(torch.int8)\n",
        "B_int4 = dense_pack_4_8(B_int8)\n",
        "\n",
        "fp16_latencies = {}\n",
        "int8_latencies = {}\n",
        "int4_latencies = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for BATCH_SIZE in tqdm([1, 2, 4, 8, 16, 32, 64, 128, 256, 512], desc=\"Iterating BATCH_SIZE\"):\n",
        "        A_fp16 = torch.randint(0, 16, (BATCH_SIZE, HIDDEN_SIZE), dtype=torch.float16, device=\"cuda\")\n",
        "        A_int8 = A_fp16.to(torch.int8)\n",
        "        A_int4 = dense_pack_4_8(A_int8)\n",
        "\n",
        "        fp16_latencies[BATCH_SIZE] = do_bench(lambda: torch.nn.functional.linear(A_fp16, B_fp16, None), rep=500)\n",
        "        int8_latencies[BATCH_SIZE] = do_bench(lambda: custom_kernel.int8_matmul(A_int8, B_int8), rep=500)\n",
        "        int4_latencies[BATCH_SIZE] = do_bench(lambda: custom_kernel.int4_matmul(A_int4.view(torch.uint8), B_int4.view(torch.uint8)), rep=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e2aa6f23f38e44dd8bd3b03f4cbdf7f9",
            "a3e518de58d84095949ad5dab0a8e345",
            "26143660de5447ab8be249ef634dd96d",
            "c66e193de38d4028bbe8a095dd630f22",
            "81942199469a4467a1a5388c66b5d0e7",
            "d2c6cc9f0a4148aa8be74d94f2ce6920",
            "51d2b87182704b57ae4462a073f4e323",
            "0c10c01ed6fd4a268702f22d8e0bb2c7",
            "e34ce1fc4ab541769042b489a07c1d44",
            "464a5860e668481fbaab8da5f56abb80",
            "fc26123cd44d4ddf8095175b1477e3ee"
          ]
        },
        "id": "iEL-EAVw9Zoo",
        "outputId": "5a4a76a1-549a-4680-b2da-cba914cfa720"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Iterating BATCH_SIZE:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2aa6f23f38e44dd8bd3b03f4cbdf7f9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(fp16_latencies.keys(), [key/val for key, val in fp16_latencies.items()], label=\"FP16\")\n",
        "plt.plot(int8_latencies.keys(), [key/val for key, val in int8_latencies.items()], label=\"INT8\")\n",
        "plt.plot(int4_latencies.keys(), [key/val for key, val in int4_latencies.items()], label=\"INT4\")\n",
        "# plt.xscale(\"log\")\n",
        "\n",
        "plt.xlabel(\"Batch size\")\n",
        "plt.ylabel(\"Troughput, elements per second\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "d1AYFgonBSUp",
        "outputId": "55e5749e-64cb-4f34-bd37-66f8189c02ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easy to see that the speedup is ~2x for INT8 and ~4x for INT4 compared to FP16 for all batch sizes!"
      ],
      "metadata": {
        "id": "gf41qwHIDLL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speculative Decoding"
      ],
      "metadata": {
        "id": "Wj8ikoVEtDlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Throughput Benchmarking"
      ],
      "metadata": {
        "id": "eOpNnZG84cTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = get_llama_model_and_tokenizer()"
      ],
      "metadata": {
        "id": "LGSWYgZ5tGTY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tABKQ69wqKk6"
      },
      "source": [
        "**Compile model forward for more accurate benchmarks**\n",
        "\n",
        "`transformers` is not a very efficient inference engine because of high python overhead and almost no kernel optimization.\n",
        "\n",
        "However, with `PyTorch` `v2.0.0`, [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) was introduced. This feature allows for capturing, isolating and optimizing CUDA runtime in `PyTorch`. Using this feature, we can effectively eliminate almost all python overhead and optimize the kernels.\n",
        "\n",
        "Starting with [`transformers` `v4.44.0`](https://github.com/huggingface/transformers/releases/tag/v4.44.0), this feature is integrated with `transformers` text generation utils end-to-end. However, for simplicity, we'll apply it to the forward pass of the model specifically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-QLsL7m0qKk6"
      },
      "outputs": [],
      "source": [
        "model = model.to(\"cuda\")\n",
        "model.forward = torch.compile(\n",
        "    model.forward,          # the function call to compile\n",
        "    fullgraph=True,         # Compile all the CUDA kernels into a single entity\n",
        "    mode=\"reduce-overhead\", # Optimize for speed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x5ojJbrqKk7"
      },
      "source": [
        "**Benchmark forward passes with different seq_len**\n",
        "\n",
        "Run the following cell 2+ times. The first time is slow because that's when the compilation is taking place.\n",
        "\n",
        "The following runs are much faster.\n",
        "\n",
        "(EXTRA: run this cell without compiling to measure scompilation speedup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y_geHUkqKk7",
        "outputId": "931bfc14-f44f-4058-f89a-6276222342c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 tokens: 80.59 passes/s\n",
            "2 tokens: 80.96 passes/s\n",
            "4 tokens: 78.80 passes/s\n",
            "8 tokens: 78.49 passes/s\n",
            "16 tokens: 71.70 passes/s\n",
            "32 tokens: 64.87 passes/s\n",
            "64 tokens: 58.71 passes/s\n",
            "128 tokens: 44.29 passes/s\n",
            "256 tokens: 24.69 passes/s\n",
            "512 tokens: 12.83 passes/s\n",
            "1024 tokens: 6.36 passes/s\n"
          ]
        }
      ],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "NUM_REPEATS = 100\n",
        "\n",
        "throughpus = {}\n",
        "\n",
        "for seq_len in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
        "    input_ids = torch.randint(0, tokenizer.vocab_size, (1, seq_len)).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            model(\n",
        "                input_ids,\n",
        "                use_cache=False,\n",
        "            )\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        start = perf_counter()\n",
        "        for _ in range(NUM_REPEATS):\n",
        "            model(\n",
        "                input_ids,\n",
        "                use_cache=False,\n",
        "            )\n",
        "            torch.cuda.synchronize()\n",
        "        end = perf_counter()\n",
        "    throughpus[seq_len] = NUM_REPEATS * seq_len / (end - start)\n",
        "    print(f\"{seq_len} tokens: {NUM_REPEATS / (end - start):.2f} passes/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-qz7ahOqKk8"
      },
      "source": [
        "As we can see, the forward pass speed almost doesn't depend on the number of tokens passed through the model up to around **16** tokens at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PwAeHLbAqKk9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "5cb68a8d-f67e-41ae-effe-ae1156e86746"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Troughput, tokens per second')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(throughpus.keys(), throughpus.values())\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "\n",
        "plt.xlabel(\"Tokens in pass\")\n",
        "plt.ylabel(\"Troughput, tokens per second\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "mngHDFBJyBRn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFIT6AmUqKk9"
      },
      "source": [
        "## Speculative Decoding\n",
        "\n",
        "As a baseline, we'll generate hypotheses using a very simple bigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7JF2jQqKk-"
      },
      "source": [
        "### Load the Data\n",
        "\n",
        "We'll use the [wikitext2](https://paperswithcode.com/dataset/wikitext-2) dataset as a sample of natural language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rk8KcWC4qKk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b82444-e207-4664-c1a9-0a5b7c307f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "def get_wikitext2(seed, seqlen, nsamples=64):\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    train_input_ids = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt').input_ids\n",
        "    random.seed(seed)\n",
        "    train_batch = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, train_input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = train_input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        train_batch.append(inp[0])\n",
        "\n",
        "    test_input_ids = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt').input_ids\n",
        "    test_input_ids = test_input_ids[:, :(test_input_ids.shape[1] // seqlen) *  seqlen]\n",
        "    test_input_ids = test_input_ids.reshape(test_input_ids.shape[1] // seqlen, seqlen)\n",
        "\n",
        "    return torch.stack(train_batch), test_input_ids\n",
        "\n",
        "train_batch, test_input_ids = get_wikitext2(SEED, 8192)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K-tFOxyqKk-"
      },
      "source": [
        "**Task (1.0pt):** Build a bigram model\n",
        "\n",
        "Using sequences from `train_batch`, build a bigram model for predicting `n` tokens into the future.\n",
        "\n",
        "WARNING: torch.Tensors have weird interactions with dicts and Counters. use `.item()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Anchr3_TqKk-"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from typing import Mapping\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def build_next_token_array(train_data, vocab_size: int=128256, default_next_token: int=220) -> Mapping[int, int]:\n",
        "    \"\"\"\n",
        "    Builds an array mapping each token in the vocabulary to its most likely next token based on training data.\n",
        "\n",
        "    Args:\n",
        "        train_data (torch.Tensor): Array of training data tokens.\n",
        "        vocab_size (int): The size of the vocabulary.\n",
        "        default_next_token (int): Default token to use if no next token can be determined.\n",
        "\n",
        "    Returns:\n",
        "        Mapping[int, int]: Array where each index represents a token and the value at that index is the most likely next token.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    # Step 1: Prepare the Data\n",
        "    tokens = train_data.tolist() if isinstance(train_data, torch.Tensor) else list(train_data)\n",
        "\n",
        "    # Step 2: Create Pairs and Count Occurrences\n",
        "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "      current_token = tokens[i].item() if isinstance(tokens[i], torch.Tensor) else tokens[i]\n",
        "      next_token = tokens[i + 1].item() if isinstance(tokens[i + 1], torch.Tensor) else tokens[i + 1]\n",
        "      bigram_counts[current_token][next_token] += 1\n",
        "\n",
        "    # Step 3: Build Mapping from Current Token to Next Token Counts\n",
        "    next_tokens_array = {}\n",
        "\n",
        "    # Step 4: Determine the Most Likely Next Token\n",
        "\n",
        "    for current_token in range(vocab_size):\n",
        "      if current_token in bigram_counts:\n",
        "          # Находим токен с максимальной частотой\n",
        "          next_token_counts = bigram_counts[current_token]\n",
        "          most_likely_next = max(next_token_counts.items(), key=lambda x: x[1])[0]\n",
        "          next_tokens_array[current_token] = most_likely_next\n",
        "      else:\n",
        "          # Если токен не встречался в обучающих данных, используем дефолтный\n",
        "          next_tokens_array[current_token] = default_next_token\n",
        "\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "\n",
        "    return next_tokens_array\n",
        "\n",
        "def speculate_bigram(input_ids: torch.Tensor, position: int, n: int, next_tokens_array: Mapping[int, int]) -> int:\n",
        "    \"\"\"\n",
        "    Generates a speculative sequence by predicting next tokens in a sequence using a bigram model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tensor of input token IDs.\n",
        "        position (int): Position in the sequence to begin speculation.\n",
        "        n (int): Number of tokens to generate.\n",
        "        next_tokens_array (Mapping[int, int]): Mapping of tokens to their most likely next token.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of tokens generated.\n",
        "    \"\"\"\n",
        "    for i in range(n):\n",
        "        hypo_next_token = next_tokens_array[input_ids[0, position - 1].item()]\n",
        "        input_ids[0, position] = hypo_next_token\n",
        "        position += 1\n",
        "    return n\n",
        "\n",
        "\n",
        "NEXT_TOKEN_WIKI2 = build_next_token_array(train_batch.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZP5SyfXqKk_"
      },
      "source": [
        "**Task (2.0pt):** Implement greedy sequential speculative decoding:\n",
        "\n",
        "You're given a prototype of the function that generate a token sequence greedily by speculating `n` tokens into the fulure and verifying those tokens.\n",
        "\n",
        "Your task is to:\n",
        " * Correctly fill a hypothesis inplace (using `speculate_fn`)\n",
        " * Pass the hypothesis through the model (with correct `past_key_values`)\n",
        " * Find where the hypothesis diverges.\n",
        " * Update the current position in the generation, as well as the number of forward pass calls."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = get_llama_model_and_tokenizer()\n",
        "model = model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "yiu9ItbT0Wvs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "MjF97B3gqKk_",
        "outputId": "b6207735-02ba-4f76-f20e-f4b352688bcf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'real_tokens' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-860857411.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m# Call the function with parameters and lambda function for speculative decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m output, forward_passes = generate_speculative(\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-860857411.py\u001b[0m in \u001b[0;36mgenerate_speculative\u001b[0;34m(model, tokenizer, num_tokens_to_generate, speculate_fn, max_speculated_len, initial_prompt, verbose)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# YOUR CODE HERE>>>>>>>>>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_tokens_speculated\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch_len\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmatch_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch_len\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'real_tokens' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "from transformers import StaticCache\n",
        "\n",
        "\n",
        "def truncate_past_key_values(past_key_values, position):\n",
        "    \"\"\"\n",
        "    Truncates the past key and value caches at a specific position. `transformers`-specific stuff. Might change with their next update.\n",
        "\n",
        "    Args:\n",
        "        past_key_values (object): Object containing key and value caches for each layer.\n",
        "        position (int): Position from which to truncate the caches.\n",
        "    \"\"\"\n",
        "    for layer in past_key_values.layers:\n",
        "        layer.keys[:,:,position - 1:] = 0.0\n",
        "        layer.values[:,:,position - 1:] = 0.0\n",
        "\n",
        "\n",
        "def generate_speculative(model, tokenizer, num_tokens_to_generate: int, speculate_fn: callable, max_speculated_len: int=128, initial_prompt: str=\"The Pacific\", verbose: bool=False):\n",
        "    \"\"\"\n",
        "    Generates text using speculative decoding, a technique that combines conventional forward passes with speculative predictions to reduce computation by hypothesizing multiple tokens at each step.\n",
        "\n",
        "    Args:\n",
        "        model (transformers.PreTrainedModel): The language model used to generate text.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer corresponding to the model for encoding and decoding text.\n",
        "        num_tokens_to_generate (int): The number of tokens to generate in total.\n",
        "        speculate_fn (callable): A function that generates speculative tokens based on the input IDs and current position.\n",
        "            This function takes `input_ids` and `position` as arguments and returns the number of tokens speculated.\n",
        "        max_speculated_len (int, optional): The maximum length of speculative tokens allowed in a single step. Defaults to 128.\n",
        "        initial_prompt (str, optional): The starting prompt for text generation. Defaults to \"The Pacific\".\n",
        "        verbose (bool, optional): If True, prints debugging information about successful speculative predictions. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The generated sequence of input IDs up to the generated position.\n",
        "        int: The number of forward passes required for generation, which can be used to assess efficiency.\n",
        "\n",
        "    Example:\n",
        "        >>> output, forward_passes = generate_speculative(\n",
        "        ...     model,\n",
        "        ...     tokenizer,\n",
        "        ...     num_tokens_to_generate=100,\n",
        "        ...     speculate_fn=lambda input_ids, position: fill_hypothesis(input_ids, position, n=2, next_tokens_array=next_tokens_array),\n",
        "        ...     verbose=True\n",
        "        ... )\n",
        "        >>> print(\"Generated Text:\", tokenizer.decode(output))\n",
        "        >>> print(\"Tokens per forward pass:\", 100 / forward_passes)\n",
        "\n",
        "    Notes:\n",
        "        - Speculative decoding reduces the number of model passes by hypothesizing tokens using the `speculate_fn` function. The actual model output is used to verify these hypotheses, allowing efficient token generation.\n",
        "        - If verbose mode is enabled, it will display speculative matches as they occur.\n",
        "    \"\"\"\n",
        "\n",
        "    # Encode the initial prompt and get input IDs, then move them to the GPU\n",
        "    prompt_ids = tokenizer(initial_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda\")\n",
        "    position = prompt_ids.shape[1]  # Initial position for the prompt\n",
        "\n",
        "    # Calculate the maximum cache length to accommodate generated tokens and speculative tokens\n",
        "    max_cache_len = num_tokens_to_generate + position + max_speculated_len\n",
        "    # Initialize the cache for past key values with model configuration, setting cache size and device\n",
        "    past_key_values = StaticCache(config=model.config, max_batch_size=1, max_cache_len=max_cache_len, device=\"cuda\", dtype=torch.float16)\n",
        "    # Allocate space for input IDs and cache positions on the GPU\n",
        "    input_ids = torch.zeros((1, max_cache_len), device=\"cuda\", dtype=torch.long)\n",
        "    # Fill in the initial prompt\n",
        "    input_ids[0, :position] = prompt_ids[0, :position]\n",
        "    with torch.no_grad():  # Disable gradients for inference\n",
        "        # Pre-fill cache with the prompt to start the model's internal state\n",
        "        past_key_values = model(input_ids[:, :position], past_key_values=past_key_values).past_key_values\n",
        "\n",
        "        forward_passes = 0  # Track number of forward passes\n",
        "        tokens_generated = 0  # Track number of tokens generated\n",
        "        while tokens_generated < num_tokens_to_generate:\n",
        "            # Adjust cache by removing outdated values to avoid memory overflow\n",
        "            truncate_past_key_values(past_key_values, position)\n",
        "            # Speculate the next few tokens based on current position\n",
        "            num_tokens_speculated = speculate_fn(input_ids, position)  # Number of tokens hypothesized\n",
        "            # YOUR CODE HERE>>>>>>>>>\n",
        "            # Обрабатываем случай когда нет спекуляции\n",
        "            if num_tokens_speculated == 0:\n",
        "                # Без спекуляции - делаем обычный forward pass для одного токена\n",
        "                # Используем последний токен для предсказания следующего\n",
        "                output = model(\n",
        "                    input_ids[:, position-1:position],\n",
        "                    past_key_values=past_key_values\n",
        "                )\n",
        "            else:\n",
        "                output = model(\n",
        "                    input_ids[:, position:position + num_tokens_speculated],\n",
        "                    past_key_values=past_key_values\n",
        "                )\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "            past_key_values = output.past_key_values  # Update the cache with new predictions\n",
        "\n",
        "            # Extract predicted tokens and the speculative tokens for comparison\n",
        "            # YOUR CODE HERE>>>>>>>>>\n",
        "            if num_tokens_speculated == 0:\n",
        "                # Без спекуляции - берем предсказание для одного токена\n",
        "                pred_tokens = output.logits[0, -1].argmax().unsqueeze(0)  # [1]\n",
        "                real_tokens = torch.tensor([], device=input_ids.device, dtype=torch.long)\n",
        "                match_len = 0\n",
        "            else:\n",
        "                # Модель предсказывает следующий токен для каждого входного токена\n",
        "                # pred_tokens[i] - это предсказание для input_ids[position+i], которое должно быть input_ids[position+i+1]\n",
        "                pred_tokens = output.logits.argmax(dim=-1).squeeze(0)  # [num_speculated]\n",
        "\n",
        "                # Сравниваем предсказания модели со спекулированными токенами\n",
        "                # real_tokens - это спекулированные токены, которые мы заполнили\n",
        "                real_tokens = input_ids[0, position:position + num_tokens_speculated]\n",
        "\n",
        "                # Сравниваем: pred_tokens[i] должен быть равен real_tokens[i+1] (если i+1 < num_tokens_speculated)\n",
        "                # Или проще: сравниваем pred_tokens[:-1] с real_tokens[1:]\n",
        "                if num_tokens_speculated > 1:\n",
        "                    matches = (pred_tokens[:-1] == real_tokens[1:])\n",
        "                else:\n",
        "                    matches = torch.tensor([], dtype=torch.bool, device=input_ids.device)\n",
        "\n",
        "                if len(matches) > 0 and matches.all():\n",
        "                    # Все предсказания совпали со спекулированными токенами\n",
        "                    match_len = num_tokens_speculated - 1\n",
        "                else:\n",
        "                    # Находим индекс первого несовпадения\n",
        "                    if len(matches) > 0:\n",
        "                        first_mismatch = (~matches).nonzero(as_tuple=True)[0]\n",
        "                        match_len = first_mismatch[0].item() if len(first_mismatch) > 0 else 0\n",
        "                    else:\n",
        "                        match_len = 0\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "            # Optionally, print successful speculative predictions\n",
        "            if verbose and match_len > 0:\n",
        "                print(\n",
        "                    f\"HIT: '{tokenizer.decode(real_tokens[:match_len])}'->'{tokenizer.decode(real_tokens[:match_len + 1])}'\"\n",
        "                )\n",
        "\n",
        "            # Copy matched tokens to the input IDs array\n",
        "            # YOUR CODE HERE>>>>>>>>>\n",
        "            if num_tokens_speculated > 0:\n",
        "                input_ids[0, position:position + match_len] = real_tokens[:match_len]\n",
        "                input_ids[0, position + match_len] = pred_tokens[match_len]\n",
        "            else:\n",
        "                input_ids[0, position] = pred_tokens[0]\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "            # Update the position, tokens generated count, and forward passes\n",
        "            # YOUR CODE HERE>>>>>>>>>\n",
        "            position += match_len + 1\n",
        "            forward_passes += 1\n",
        "            tokens_generated += match_len + 1\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "    return input_ids[0, :position + num_tokens_to_generate - tokens_generated], forward_passes\n",
        "\n",
        "\n",
        "NUM_TOKENS_TO_GENERATE = 100\n",
        "\n",
        "# Call the function with parameters and lambda function for speculative decoding\n",
        "output, forward_passes = generate_speculative(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    NUM_TOKENS_TO_GENERATE,\n",
        "    speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, n=2, next_tokens_array=NEXT_TOKEN_WIKI2),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Display metrics and the decoded output\n",
        "print(f\"Tokens per forward pass: {NUM_TOKENS_TO_GENERATE / forward_passes}\\n\")\n",
        "print(tokenizer.decode(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vSbqLx7lqKlA",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "4a7e8e1c-1bbe-4867-ff68-5f31ff980f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occured. Generated texts:\n",
            "\n",
            "<|begin_of_text|>The Pacific Northwest is a region of the United States that includes the states of Washington, Oregon, and Idaho. It is known for its natural beauty, including the Cascade Mountains, the Pacific Ocean, and the Columbia River. The region is also home to a diverse population of people, including Native Americans, Asians, and Europeans. The Pacific Northwest is a region of the United States that includes the states of Washington, Oregon, and Idaho. It is known for its natural beauty, including the Cascade Mountains, the Pacific\n",
            "<|begin_of_text|>The Pacificic is new boat a 1 9 , \n",
            "   200\n",
            "200 200 7200 77,  7 ,200 77,200,200,7,7 and,,, and and and and and and the and and the the7,, and and and the and 7 and and the the200,7 and and ,, and , and and and the the7 and and the and  and  and \n",
            "<|begin_of_text|>The Pacificic is new boat a 1 9 , \n",
            "   200\n",
            "200 200 7200 77,  7 ,200 77,200,200,7,7 and,,, and and and and and and the and and the the7,, and and and the and 7 and and the the200,7 and and ,, and , and and and the the7 and and the and  and  and \n",
            "<|begin_of_text|>The Pacificic is new boat a 1 9 , \n",
            "   200\n",
            "200 200 7200 77,  7 ,200 77,200,200,7,7 and,,, and and and and and and the and and the the7,, and and and the and 7 and and the the200,7 and and ,, and , and and and the the7 and and the and  and  and \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "It's very unlikely that the performance decreases with stronger speculation",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1924680069.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error occured. Generated texts:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1924680069.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mn0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNUM_TOKENS_TO_GENERATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Model that doesnt' speculate does exatcly one forward pass per token\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mn4\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mn2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"It's very unlikely that the performance decreases with stronger speculation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutput0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The outputs diverge too early. Maybe ignore if really sure it's okay\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: It's very unlikely that the performance decreases with stronger speculation"
          ]
        }
      ],
      "source": [
        "output0, n0 = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, 0, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "output1, n1 = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, 1, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "output2, n2 = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, 2, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "output4, n4 = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, 4, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "\n",
        "try:\n",
        "    assert n0 == NUM_TOKENS_TO_GENERATE, \"Model that doesnt' speculate does exatcly one forward pass per token\"\n",
        "    assert n4 <= n2 and n2 <= n1 and n1 < n0, \"It's very unlikely that the performance decreases with stronger speculation\"\n",
        "    assert (output1[:40] == output0[:40]).all(), \"The outputs diverge too early. Maybe ignore if really sure it's okay\"\n",
        "    print(\"All tests passed\")\n",
        "except AssertionError as e:\n",
        "    print(\"Error occured. Generated texts:\\n\")\n",
        "    print(*tokenizer.batch_decode([output0, output1, output2, output4]), sep=\"\\n\", end=\"\\n\\n\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIplTHc7qKlA"
      },
      "outputs": [],
      "source": [
        "for speculate_length in [0, 1, 2, 3, 4, 5, 6]:\n",
        "    _, num_passes = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, speculate_length, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "    print(f\"Speculate length {speculate_length}: {NUM_TOKENS_TO_GENERATE / num_passes:.2f} tokens per forward pass\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T48EpfDmqKlA"
      },
      "source": [
        "It appears the bigram model doesn't improve above `2`. That is because it's very unlikely that a bigram model outputs a long meaningful sequence. For better models and approaches, read this:\n",
        " - https://arxiv.org/abs/2503.01840\n",
        " - https://arxiv.org/abs/2305.09781\n",
        " - https://arxiv.org/abs/2406.02532"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQkjY8Jp1OpH"
      },
      "source": [
        "# Quantizing Matrices Row-Wise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzEoWTNMi62K"
      },
      "source": [
        "### Basic Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFG1FDsVi62L"
      },
      "source": [
        "**Mapping the values to the allowed range**\n",
        "\n",
        "Quantization is the process of mapping input values from a large set to output values in a smaller set. For instance, if we consider 4-bit\n",
        "quantization, our values are represented by $4$ bits, meaning we can represent values between 0 and $2^4-1=15$.\n",
        "\n",
        " * To produce the quantized representation, we need to be able to map the matrix values to and from this range.\n",
        " * For reasons that become important later, we will perform this mapping independently for each matrix row.\n",
        " * We will parametrize the mapping like this: $out = \\frac{in}{scale} + zero$, where $scale$ and $zero$ are row-wise constants.\n",
        " * For a matrix of size `(m, k)` ($m$ rows, $k$ columns) we will aggregate those parameters into two vectors `scale` and `zero` of size `(m, 1)`.\n",
        "\n",
        "**Task (0.5pt):** Complete the function below to perform this mapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1xDv2d12hw9"
      },
      "outputs": [],
      "source": [
        "def get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produce tensors scale and zero of shape (m, 1)\n",
        "        such that 0 < x / scale + zero < max_abs\"\"\"\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "    return scale, zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ8TZiGyi62M",
        "outputId": "823e8894-06cf-4013-c390-27cca6cced4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing your code\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "assert scale.shape == (512, 1), \"scale is wrong shape\"\n",
        "assert zero.shape == (512, 1), \"zero is wrong shape\"\n",
        "assert torch.all(scale * 15 <= 1023.1), \"Scale can't be that large. The resulting interval is too wide\"\n",
        "assert torch.all(scale * 15 >= 1022.9), \"Scale shouldn't be that small. The resulting interval is too narrow\"\n",
        "assert torch.all(-0.001 <  x / scale + zero) and torch.all(x / scale + zero < 15 + 0.001)\n",
        "\n",
        "x = torch.zeros(128, 128)\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "assert torch.all(scale == 1) and torch.all(scale * 15 >= 0.99), \"If all the values in a row are identical, let us set scale to 1\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E1DfQaSi62N"
      },
      "source": [
        "**Quantization**\n",
        "\n",
        "Having mapped the values into the allowed range, we can simply round them to obtain the quantized matrix. Complete the functions below to perform row-wise quantization. Note that:\n",
        " * You should `torch.clamp(...)` the quantized values to ensure that they are in the allowed range.\n",
        " * Some functions return the quantized matrix, as well as the quantization constants, because we'll need them to dequantize the matrix. Use `get_scale_and_zero` to obtain the them.\n",
        " * Note that we cast the quantized tensor to `uint8`, but the values themselves must be in the possibly narrower range, as determined by the number of bits. Obviously, we require the latter to be less or equal than 8.\n",
        "\n",
        "**Task (0.5pt):** Complete the function below to perform quantization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZR0OZ0ai62N"
      },
      "outputs": [],
      "source": [
        "def quantize(x: Tensor, scale: Tensor, zero: Tensor, bits: int) -> Tensor:\n",
        "    \"\"\"Quantizes a tensor\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "        bits (int): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        Tensor: quantized tensor in uint8\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "    return quantized_x.to(torch.uint8)\n",
        "\n",
        "\n",
        "def dequantize(quantized_x: Tensor, scale: Tensor, zero: Tensor) -> Tensor:\n",
        "    \"\"\"Dequantize a tensor\n",
        "    Args:\n",
        "        quantized_x (Tensor): quantized tensor in uint8\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "\n",
        "    Returns:\n",
        "        Tensor: dequantized tensor\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "\n",
        "def measure_and_quantize(x: Tensor, bits: float) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    \"\"\"Determine the values interval mapping parameters and quantize a tensor\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        bits (float): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor, Tensor]: quantized tensor, scale, zero\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "    return x_quantized, scale, zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM8eIsR8i62O",
        "outputId": "10539d13-01ef-4519-cfd8-cc179ce5070d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing your code\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "quantized_x, scale, zero = measure_and_quantize(x, 4)\n",
        "\n",
        "assert quantized_x.shape == x.shape, \"Shape of quantized_x is incorrect\"\n",
        "assert scale.shape == (512, 1), \"Shape of scale is incorrect\"\n",
        "assert zero.shape == (512, 1), \"Shape of zero is incorrect\"\n",
        "assert torch.all(quantized_x >= 0) and torch.all(quantized_x <= 15) and torch.any(quantized_x == 15), \"wrong quantized_x values range\"\n",
        "assert torch.allclose(x, dequantize(quantized_x, scale, zero), atol=50), \"Dequantized values are too far from the original values\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBDvbhJSi62O"
      },
      "source": [
        "**Using the quantized matrix**\n",
        "\n",
        "To actually use the matrix, we'll have to map it's values back into their original form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzGxzt-i62P"
      },
      "outputs": [],
      "source": [
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, quantized_weight, scale, zero, bias):\n",
        "        super().__init__()\n",
        "        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n",
        "        self.scale = nn.Parameter(scale, requires_grad=False)\n",
        "        self.zero = nn.Parameter(zero, requires_grad=False)\n",
        "        self.bias = nn.Parameter(bias.data.clone().detach()) if bias is not None else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, dequantize(self.quantized_weight, self.scale, self.zero), self.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncM_JzOii62P"
      },
      "source": [
        "This class will be used as a replacement for `nn.Linear`. It holds the quantized weight and only dequantizes it during it's forward passes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPaMiUC91VYH"
      },
      "source": [
        "# LLM Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agCFfP7x2Au6"
      },
      "source": [
        "### RTN Quantization for LLaMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwS6bTIwi62V"
      },
      "source": [
        "**Auxiliary functions:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vo0UYKvi62V"
      },
      "outputs": [],
      "source": [
        "def find_linear_layers(module: nn.Module, name='') -> dict[str, nn.Module]:\n",
        "    \"\"\"\n",
        "    Returns a dict of all nn.Linear submodules in a module with paths as keys\n",
        "    \"\"\"\n",
        "    if type(module) == nn.Linear:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_linear_layers(\n",
        "            child, name=name + '.' + name1 if name != '' else name1\n",
        "        ))\n",
        "    return res\n",
        "\n",
        "\n",
        "def replace_submodule(module, submodule_path, new_submodule):\n",
        "    \"\"\"\n",
        "    Replaces a submodule specified by a path with a new submodule\n",
        "    \"\"\"\n",
        "    submodule_names = submodule_path.split(\".\")\n",
        "    for submodule in submodule_names[:-1]:\n",
        "        module = getattr(module, submodule)\n",
        "    setattr(module, submodule_names[-1], new_submodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJALPHCSi62W"
      },
      "source": [
        "**Load-Quantize cycle**\n",
        "\n",
        "First, take a look at the function below. It uses the functions above to load the layers one by one and iterate over their `Linear` submodules replacing them with `QuantizedLinear`. A few things to keep in mind:\n",
        " * Note that the quantization happens on `.cuda()`, because we'll load *LLaMA* in `float16` which is not supported on `cpu`.\n",
        " * We call `torch.cuda.empty_cache()` after processing each layer because we'll have just enough *VRAM* for this to work.\n",
        " * The loaded model is placed in RAM.\n",
        "\n",
        "**Task (1.0pt):** implement RTN quantization for *LLaMA*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TRwggs3Lb2F"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def llama_rtn(model: LlamaForCausalLM, bits: int):\n",
        "    \"\"\"Iterates LLaMA layers one by one and quantizes them with RTN\n",
        "    Args:\n",
        "        model (LlamaForCausalLM): model to quabtuze\n",
        "        bits (int): number of bits to quantize to\n",
        "    \"\"\"\n",
        "    # Load and quantize all the layers\n",
        "    layers = model.model.layers\n",
        "    for i in trange(len(layers)):\n",
        "        # Move layer to \"cuda\" and replace all linear submodules with QuantizedLinear\n",
        "        # Use:\n",
        "        #   - find_linear_layers\n",
        "        #   - replace_submodule\n",
        "\n",
        "        layer = layers[i].cuda()\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "        layers[i] = layer.cpu()\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4OlcGF2aEts"
      },
      "source": [
        "### Testing the Quantized Model\n",
        "\n",
        "Now we have everything we need to quantize the _LLaMA-7B_ model to 4 bits. Let us do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luUgnOhxaFcw"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "BITS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3b198e32183941d5b71735bbdf6e02ad",
            "6b21652a5d324d4b9431e45b1e259f1c",
            "4a1a5f7fec1046fbbdfd59eadb061ad3",
            "958d973606864ff4bf801794b6ab0fb4",
            "b664ce1f23024e29b5cbc38cfe8c886e",
            "8c12e1dc554646d2b8785de04d393089",
            "879cb9b5251c48f5b91a833fd82f69ca",
            "72336f69aaf44b5a80d1a169eb83e4db",
            "c601f92df97f45f0a1f8fca8a32e20ed",
            "bc49b12c3f5341dea3199c7d95294cfe",
            "854c2071c81e4c4b83da4c391194fcad"
          ]
        },
        "id": "tUD4z4t7a7z8",
        "outputId": "9310fe1a-a259-4632-90d2-6457703aa914"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b198e32183941d5b71735bbdf6e02ad"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, tokenizer = get_llama_model_and_tokenizer()\n",
        "llama_rtn(model, BITS)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_names = [name for name, module in model.named_modules() if isinstance(module, torch.nn.Linear)]\n",
        "assert linear_names == ['lm_head'], f\"Only lm_head shall not be quantized, got {linear_names}\""
      ],
      "metadata": {
        "id": "unG_6fkOTEWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EN9tVawbUfG"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Can you explain the Pythagorean theorem?\",\n",
        "    \"What is photosynthesis?\",\n",
        "    \"Give me a summary of 'Romeo and Juliet'\",\n",
        "    \"How far is the moon from the Earth?\",\n",
        "    \"What is a haiku?\",\n",
        "]\n",
        "answers = []\n",
        "\n",
        "for question in tqdm(questions):\n",
        "    tokenized_input = tokenizer(\n",
        "        f\"QUESTION: {question}\\n ANSWER:\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            tokenized_input[\"input_ids\"].cuda(),\n",
        "            max_length=50, num_beams=3, early_stopping=True,\n",
        "        )[0]\n",
        "    answer = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    answers.append(answer[:answer.find(\".\")] + \".\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-QFd87GjNvd",
        "outputId": "7ed4353d-1800-4958-8f40-9a8de505e9a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION: What is the capital of France?\n",
            " ANSWER: Paris\n",
            "What is the capital of France?\n",
            "What is the capital of France?\n",
            "What is the capital of France?\n",
            "What is the capital of France.\n",
            "\n",
            "QUESTION: Can you explain the Pythagorean theorem?\n",
            " ANSWER: The Pythagorean theorem states that the distance between two points is equal to the sum of the squares of the lengths of the two points.\n",
            "\n",
            "QUESTION: What is photosynthesis?\n",
            " ANSWER: The process of photosynthesis is the use of light and energy from the sun to convert carbon dioxide and water into sugars.\n",
            "\n",
            "QUESTION: Give me a summary of 'Romeo and Juliet'\n",
            " ANSWER: 'Romeo and Juliet' is a play by William Shakespeare.\n",
            "\n",
            "QUESTION: How far is the moon from the Earth?\n",
            " ANSWER: The distance between the Earth and the moon is about 100,000 miles.\n",
            "\n",
            "QUESTION: What is a haiku?\n",
            " ANSWER: A haiku is a Japanese poem.\n"
          ]
        }
      ],
      "source": [
        "print(*answers, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn-Vko3urrM0"
      },
      "outputs": [],
      "source": [
        "model = model.cpu()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFpjGxX2H-J"
      },
      "source": [
        "### Evaluating the model\n",
        "\n",
        "Before we start quantizing the model itself, let us create a way to evaluate it's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G6Muc5ZyupZ"
      },
      "source": [
        "**Downloading the data**\n",
        "\n",
        "As a metric of the models' performance, we'll use it's PPL on the [wikitext2](https://paperswithcode.com/dataset/wikitext-2) dataset. Let us download and tokenize it. We'll need two subsets of it:\n",
        " * Test set of size ... to evaluate the models.\n",
        " * A train subset of size ... that we'll use later for GPTQ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUSNnguPysaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7df933-f209-4add-a183-36d90e772e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "SEED = 0\n",
        "train_batch, test_input_ids = get_wikitext2(SEED, 2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A77M_WBTuOLh"
      },
      "source": [
        "**Model offloading**\n",
        "\n",
        "We want to evaluate the model's performance on a large dataset. The model barely fits on the *GPU*, and we'll have to infer in on long text sequences. We don't have enought *VRAM* to do that.\n",
        "\n",
        "Instead, we'll keep most of the model in *RAM*, only transferring the layers to *GPU* as we go through them one by one, and putting them back when we're done.\n",
        "\n",
        "**Obtaining the first layer inputs**\n",
        "\n",
        "To start iterating over the layers, we'll first have to obtain the fist layer inputs. We use the function below to do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbxZfwj8ipPF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "class Catcher(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # We know that LLaMA layers take a Tensor of hidden states,\n",
        "        # and some kwargs of which `position_embeddings` is required.\n",
        "        # `position_embeddings` are also the same for the entire dataset,\n",
        "        # so we only have to register the last ones\n",
        "        self.hidden_states = []\n",
        "        self.position_embeddings = None\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states, **kwargs):\n",
        "        assert hidden_states.shape[0] == 1 # only one element from dataset\n",
        "        self.hidden_states.append(hidden_states[0])\n",
        "        self.position_embeddings = kwargs['position_embeddings']\n",
        "        raise ValueError\n",
        "\n",
        "    def get_the_catch(self):\n",
        "        return torch.stack(self.hidden_states), self.position_embeddings\n",
        "\n",
        "\n",
        "def get_first_layer_inputs(model: nn.Module, model_inputs: Tensor):\n",
        "    catcher = Catcher()\n",
        "    original_layers = model.model.layers\n",
        "\n",
        "    model.model.layers = nn.ModuleList((catcher,))\n",
        "    for sample in model_inputs:\n",
        "        try:\n",
        "            model(sample.unsqueeze(0))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    model.model.layers = original_layers\n",
        "\n",
        "    return catcher.get_the_catch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFI_nTCXi62Y"
      },
      "source": [
        "**Forward passing layer-by-layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzEPMSsEi62Y"
      },
      "outputs": [],
      "source": [
        "def forward_pass_layer(layer: nn.Module, inps: torch.Tensor, outs: torch.Tensor, position_embeddings: Tensor):\n",
        "    \"\"\"Forward pass inps through the layer ONE INP AT A TIME saving the outputs into the corresponding elements of outs\"\"\"\n",
        "    for j in range(inps.shape[0]):\n",
        "        outs[j] = layer(inps[j].unsqueeze(0), position_embeddings=position_embeddings)[0]\n",
        "\n",
        "\n",
        "def get_batch_nll(model: nn.Module, batch: Tensor):\n",
        "    # Collect the first layer inputs, put them on .cuda()\n",
        "    inps, position_embeddings = get_first_layer_inputs(model, batch)\n",
        "    inps = inps.cuda()\n",
        "    position_embeddings = (position_embeddings[0].cuda(), position_embeddings[1].cuda())\n",
        "\n",
        "    # Create a buffer for layer outputs\n",
        "    outs = torch.zeros_like(inps)\n",
        "\n",
        "    # Forward pass through the layers\n",
        "    layers = model.model.layers\n",
        "    for i in trange(len(layers), leave=False):\n",
        "        layer = layers[i].cuda() # Take a layer and put in on .cuda()\n",
        "\n",
        "        forward_pass_layer(layer, inps, outs, position_embeddings) # Forward pass a layer\n",
        "        inps, outs = outs, inps # Prepare the inputs and the output buffer for the next layer. Reuse the existing buffers\n",
        "\n",
        "        layers[i] = layer.cpu() # Put layer back on .cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    # Calculate NLL\n",
        "    nll = 0\n",
        "    model.model.norm = model.model.norm.cuda()\n",
        "    model.lm_head = model.lm_head.cuda()\n",
        "    for i in range(inps.shape[0]):\n",
        "        lm_logits = model.lm_head(model.model.norm(inps[i].unsqueeze(0)))\n",
        "        labels = batch[i]\n",
        "        # Calculate the language modeling Negative Log Likelyhood\n",
        "        shift_logits = lm_logits[:, :-1, :]\n",
        "        shift_labels = labels[1:]\n",
        "        loss = torch.nn.functional.cross_entropy(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1).cuda(),\n",
        "            reduction=\"sum\",\n",
        "        )\n",
        "        nll += float(loss)\n",
        "    model.model.norm = model.model.norm.cpu()\n",
        "    model.lm_head = model.lm_head.cpu()\n",
        "    return nll\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_eval(model, test_input_ids):\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    total_nll = 0\n",
        "    for batch in tqdm(torch.tensor_split(test_input_ids, 4)):\n",
        "        total_nll += get_batch_nll(model, batch)\n",
        "\n",
        "    # Calculate PPL\n",
        "    ppl = math.exp(total_nll / test_input_ids.numel())\n",
        "    print(f\"PPL: {ppl}\")\n",
        "    return ppl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHq3Z8RlLpuc"
      },
      "source": [
        "**Calculating PPL**\n",
        "\n",
        "We've already loaded and quantized the model. All that's left is to evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybS4iAjsq67Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "c30812599ae8463cbabdf060b9e3a95f",
            "591d665f6a3e42a4a904b4287562354e",
            "4f3c599092f84277b59c3ee513cb8a48",
            "41d3d108b0594478ad286f6eaf3ec880",
            "4587e69bfa9d45459ab9085b525a2a9a",
            "a53634e0c5284d0ab96e2729d96ebe3e",
            "468ee229badd4564a509d77e2c20f5ae",
            "56b97c3a5438457c8d5e49945ee7c359",
            "54b0fc90391b4cc090e634f5adad3b36",
            "db696e34da8245f884f8306be6d0407c",
            "cf88d16ebbde4cf9a0d9c384dd251512",
            "895361a9c29e41249079c6c219b62a60",
            "67ff1daaa4894d15a500b53fb688315d",
            "5109ed5c65af493f99ca65e8ae056341",
            "a7a76d678d34449f844e6cf562544636",
            "651404828f32450ba2617a4055208525",
            "bf1fceaae3d246f7bd987c80cde6b3a1",
            "d7d3f31d770e4d07a3c07280be9d0d76",
            "535b86e76abb4d1f9b14a3a029926ce1",
            "58ef8700d48f44658530bd94b59e0d7e",
            "b1206963f77e4d04afebdeca2f53e17b",
            "4391b0a987c242a09ca508947475fdce",
            "0b1567d2381c4c2cbf624ce2d7df5c51",
            "8c490f6e00fa40ef817693970a95ce16",
            "368a93e053ac4a8a80a3d8fbee2af59c",
            "2c777d656dd24b7a9248766cb337823f",
            "f47628915e474894a021df78e067178d",
            "dfde344aaecb4ae5b71abaedbc3d8e82",
            "85fb87e716254d01b80f31b7d677e477",
            "48fa4596ce494986adcfa9d5fd2dd237",
            "53132e713c4f4f2a9f640b38734ed57b",
            "cc470eec03b84634a8b24bb1c1629ad6",
            "1a2ddf708d274eed840a0a615a742e68",
            "47802d284cb64cdf97ea4dd8d2d745f7",
            "3e2b0a313cf04537a0c838723d101910",
            "bc442003e8654cd39aac1d0adb3a6436",
            "468ed1c3f4724ba0a1f5da79cb057e09",
            "7d5e2dc2b33043cbba5263714c9695b2",
            "d917d93b69b04d749563afbef38f7027",
            "92ddcdee533b4ae8b7856221779cf328",
            "428f1129f84743e19798d4fd5e4d2900",
            "c5d990569ae545928ae14d7bf1cf665f",
            "e3ac4cdc638d4224a24e47fcc799c0ad",
            "da6f1ebdfcbf4b57bb42d82af0d3df5c",
            "760a1d56a0914d56a0fc76f9358f579b",
            "a71ba2c0fde044e3944edf710b5ab16a",
            "9c2ae83d4f604999b53ed5d645cb1b38",
            "fa5a801574b641e69221c2a9591cc0fd",
            "44d69d8680794dff9e82334b40cdd08a",
            "5f2e24ff10ae4c438997dfc7485ce2d7",
            "52e46645c2ff4f6e9c0111679222091e",
            "54edcb8dd4ff4d83b6c00198302aa1e2",
            "4cb94e16248e4e31be8cfa089a6fd7db",
            "ae3fee6c9ea74e3c9d9ac1cdbcaf6c2c",
            "3fc1401ce01748a9ae947df38c597680"
          ]
        },
        "outputId": "0793dbce-92b8-4cf8-a5a2-c9f935fa9b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c30812599ae8463cbabdf060b9e3a95f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "895361a9c29e41249079c6c219b62a60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b1567d2381c4c2cbf624ce2d7df5c51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47802d284cb64cdf97ea4dd8d2d745f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "760a1d56a0914d56a0fc76f9358f579b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPL: 19.077815206794014\n"
          ]
        }
      ],
      "source": [
        "rtn_ppl = llama_eval(model, test_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "722oBDnwq_1x"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "\n",
        "assert rtn_ppl > 18 and rtn_ppl < 20\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urb_7OhSq_zA"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQIk33arN6A"
      },
      "source": [
        "### GPTQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t16SaWf3rQt7"
      },
      "source": [
        "GPTQ is the State Of The Art quantization algorithm for post-training DL model quantization. It works by sequentially quantizing the model's linear layer weights.\n",
        "\n",
        "Although in outputs results similar to what one would get with Round To Nearest quantization, it makes a key observations to boost it's end performance:\n",
        " * It is layer input aware (also referred to as \"1-Shot\" method), meaning int optimizes the quantized matrix to show best performance on inputs typically encountered in that layer.\n",
        "More formally, the problem can be formulated as:\n",
        "$$\n",
        "W_q = argmin_{\\widehat{W}}\\|XW^T - X\\widehat{W}^T\\|_2^2\n",
        "$$\n",
        ", where\n",
        " * $X$ is the input matrix of shape `(..., IN)`.\n",
        " * $XW^T$ is the unquantized output of shape `(..., OUT)`. We think of the norm above as taking a sum over those (...) dimensions.\n",
        " * $W$ is the unquantized weight of shape `(OUT, IN)`.\n",
        " * $\\widehat{W}$ is the quantized weight taken from some quantization grid.\n",
        "\n",
        "One can notice that the expression above is independent with regard to the rows of $W$ and $\\widehat{W}$, meaning we can solve it for each row in parallel. This is the reason why we're working with row-wise quantization in the first place. Notice that the quantization grid only depends on min/max values withing the row and not the quantization process, so we can think of it as fixed.\n",
        "\n",
        "and the dimension of the optimization problem is `IN`, which is too much to solve exactly. The algorithm proposes to solve it iteratively.\n",
        "\n",
        "Less us consider a vector of full precision weights $F$ and corresponding sent of inputs $X_F$. The corresponding objective is quadratic with Hessian\n",
        "$$\n",
        "H_F = 2X_F^TX_F^.\n",
        "$$\n",
        "The algorithm can be described like this:\n",
        " * Do the following steps until $F$ is fully quantized:\n",
        "    1. Given the next index to quantize $i$, and corresponding unquantized element $F_i$.\n",
        "    2. Quantize the coordinate by prjecting in onto the quantization grid $Q_i = quant(F_i)$.\n",
        "    3. Update all of the remaining weights $F_: = F_: - \\frac{F_i - quant(F_i)}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,:}$.\n",
        "    4. Exclude $i$ from $F$.\n",
        "\n",
        "It uses the inverse Hessian to slightly tune the remaining unquantized weights to mitigate the quantization error.\n",
        "\n",
        "As for how $i$ is chosen, an observation was made that iterating over indices in order of **decreasing diagonal Hessian elements** provides the best performance.\n",
        "\n",
        "There are a few more ideas that make this algorithm much faster:\n",
        " 1. We can represent the order of quantization (selection of $i$) by permuting the row in advance, and then iterating over the row element in order.\n",
        " $$\n",
        "   F_{i:} = F_{i:} - \\frac{F_{i} - quant(F_{i})}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,i:}\n",
        " $$\n",
        " 2. The problem is row-wise independent, meaning that we use the same permutation each row and perform those operations in a vector fashion for all the rows at the same time.\n",
        " $$\n",
        "   F_{:,i:} = F_{:,i:} - \\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\odot\\left[H_F^{-1}\\right]_{i,i:}\\leftarrow\\text{\\textbf{ you'll have to code this}}\n",
        " $$\n",
        " 3. We don't actually need to recompute the inverse Hessian. At $i$-th step we only need its $t$-th row, and we can use fancy math to precompute the matrix containing all of those rows in advance.\n",
        " $$\n",
        "  H^{-1} = Cholesky(H^{-1})^T    \n",
        " $$\n",
        "\n",
        " 4. We don't need to tune all the remaining unquantized values right away. We can only apply the updates for the closest elements right away and accumulate all the other updates to apply them only once in a while.\n",
        "\n",
        "    We'll do this in block of fixed size, applying the updates inside of those blocks and updating the weights outside only when we're done with the block. To accumulate those updates, we'll collect the scaled quantization error\n",
        "    $$\n",
        "      Err_{:,i} =\\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\text{ for all }i\\text{ in block}.\n",
        "    $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogTj9Op5raaD"
      },
      "source": [
        "**GPTQ within blocks**\n",
        "\n",
        "Implement GPTQ within the block. Iterate over the columns in ordered vector fashion, quantizing them one by one and updating all the remaining colums within the block.\n",
        "\n",
        "Return the quantized weight as well as the matrix of quantization errors that we'll need to tune the unquantized weights outside of the block.\n",
        "\n",
        "**Task (2.0pt):** Implement GPTQ block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8GwQU5CrNv_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def gptq_block(block_weight: Tensor, block_hessian_inverse: Tensor, scale: Tensor, zero: Tensor, bits: int) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Perform GPTQ within block\n",
        "    Args:\n",
        "        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n",
        "        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n",
        "        scale (Tensor): row-wise quantization constats of shape (OUT, 1)\n",
        "        zero (Tensor): row-wise quantization constats of shape (OUT, 1)\n",
        "        bits (int): number of bits to quantize() to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n",
        "    \"\"\"\n",
        "    block_weight = block_weight.clone()\n",
        "    quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.uint8, device=block_weight.device)\n",
        "    scaled_block_error = torch.zeros_like(block_weight)\n",
        "\n",
        "    # Interate over the block's columns\n",
        "    for i in range(block_weight.shape[1]):\n",
        "        # Get the column and the corresponding inverse Hessian\n",
        "        column_weight = block_weight[:, [i]]\n",
        "        hessian_slice = block_hessian_inverse[[i], i:]\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    return quantized_block_weight, scaled_block_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgRy6B_didyY"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "!wget -O gptq_block_weight_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_weight_reference.pt --no-check-certificate\n",
        "!wget -O gptq_block_error_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_error_reference.pt --no-check-certificate\n",
        "\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(0)\n",
        "\n",
        "weight = torch.rand((128, 128), generator=generator).cuda()\n",
        "scale, zero = get_scale_and_zero(weight, 15)\n",
        "\n",
        "block_weight = weight[:,:16]\n",
        "\n",
        "block_hessian_inverse = (torch.triu(torch.rand((16, 16), generator=generator), diagonal=1) + torch.diag(torch.rand(16, generator=generator) + 1)).cuda()\n",
        "quantized_block_weight, scaled_block_error = gptq_block(block_weight, block_hessian_inverse, scale, zero, 4)\n",
        "\n",
        "assert torch.all(quantized_block_weight == torch.load(\"gptq_block_weight_reference.pt\"))\n",
        "assert torch.allclose(scaled_block_error, torch.load(\"gptq_block_error_reference.pt\"), rtol=1e-5, atol=1e-06)\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpXmzbg8y18R"
      },
      "source": [
        "**Now we can implement the full algorithm:**\n",
        " * Get row-wise quantization constants.\n",
        " * Sort the columns by decreasing Hessian diagonal values. Think about how you'd have to permute the Hessian as well.\n",
        " * Process the Hessian to obtain the precomputed inverse Hessian.\n",
        " * Iterate over the columns in blocks:\n",
        "    * Get the next block and quantize it.\n",
        "    * Tune all the following blocks to mitigate the quantization error.\n",
        "      $$\n",
        "         F_{:,block\\_end:} = F_{:,block\\_end:} - Err_{:,block\\_start:block\\_end}\\odot\\left[H_F^{-1}\\right]_{block\\_start:block\\_end,block\\_end:}\n",
        "      $$\n",
        " * Restore the original order for quantized columns.\n",
        "\n",
        "**Task (2.0pt):** implement the full algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqI4FamprFPE"
      },
      "outputs": [],
      "source": [
        "def prepare_inverse_hessian(hessian: Tensor, percdamp: float) -> Tensor:\n",
        "    \"\"\"Precomputes inverse Hessian\n",
        "    Args:\n",
        "        hessian (Tensor): problem hessian\n",
        "        percdamp (float): diagonal damping constant for numerical stability\n",
        "    Returns:\n",
        "        Tensor: precomputed inverse Hessian\n",
        "    \"\"\"\n",
        "    damp = percdamp * torch.mean(torch.diag(hessian))\n",
        "    diag = torch.arange(hessian.shape[0], device=weight.device)\n",
        "    hessian[diag, diag] += damp\n",
        "    hessian = torch.linalg.cholesky(hessian)\n",
        "    hessian = torch.cholesky_inverse(hessian)\n",
        "    hessian = torch.linalg.cholesky(hessian, upper=True)\n",
        "    return hessian\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def gptq(weight: torch.Tensor, bits: int, hessian: torch.Tensor, blocksize:int=128, percdamp:float=.01) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    \"\"\"Quantizes weight with GPTQ\n",
        "    Args:\n",
        "        weight (torch.Tensor): weight to quantize\n",
        "        bits (int): number of bits to quantize to\n",
        "        hessian (torch.Tensor): problem Hessian\n",
        "        blocksize (int, optional): Defaults to 128.\n",
        "        percdamp (float, optional): Hessian damping constant for numerical stability. Defaults to .01.\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor, Tensor]: quantized_weight, row-wise quantization scales, row-wise quantization zeroes\n",
        "    \"\"\"\n",
        "    dtype = weight.dtype\n",
        "    weight = weight.clone().detach()\n",
        "    weight = weight.float()\n",
        "    num_columns = weight.shape[1]\n",
        "    hessian = hessian.float()\n",
        "\n",
        "    # Identify and patch always-zero input coordinates\n",
        "    dead = torch.diag(hessian) == 0\n",
        "    hessian[dead, dead] = 1\n",
        "    weight[:, dead] = 0\n",
        "\n",
        "    # Get row-wise quantization constants\n",
        "    scale, zero = get_scale_and_zero(weight, 2 ** bits - 1)\n",
        "\n",
        "    # Sort the columns by decreasing Hessian diagonal values.\n",
        "    # Transform the hessian accordingly.\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    perm = ...\n",
        "    invperm = ...\n",
        "\n",
        "    hessian = ...\n",
        "    weight = ...\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    # Process the Hessian to obtain the precomputed inverse Hessian\n",
        "    hessian_inverse = prepare_inverse_hessian(hessian, percdamp)\n",
        "\n",
        "    # Iterate over the columns in blocks\n",
        "    quantized_weight = torch.zeros(weight.shape, dtype=torch.uint8, device=weight.device)\n",
        "    for block_start in range(0, num_columns, blocksize):\n",
        "\n",
        "        block_end = min(block_start + blocksize, num_columns)\n",
        "\n",
        "        # Get the next block and quantize it\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "\n",
        "        # Tune all the following blocks to mitigate the quantization error\n",
        "        weight[:, block_end:] = ...\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    # Reverse the permutation of the quantized weight\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    quantized_weight = ...\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    return quantized_weight, scale.to(dtype), zero.to(dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI1R_stdrFMk"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "!wget -O gptq_weight_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_weight_reference.pt --no-check-certificate\n",
        "\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(0)\n",
        "\n",
        "hessian = torch.triu(torch.rand((128, 128), generator=generator) + 4 * torch.eye(128)).cuda().half()\n",
        "hessian += hessian.clone().T\n",
        "quantized_weight, _, _ = gptq(weight, 4, hessian, 32)\n",
        "\n",
        "assert torch.all(quantized_weight == torch.load(\"gptq_weight_reference.pt\"))\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHAnRizBy8HC"
      },
      "source": [
        "**Sequential Model Quantization**\n",
        "\n",
        "The GPT quantization approach implemented here requires an ordered approach due to its input-dependent nature. For each `Linear` submodule within the GPT model, we need to ensure that the input data is representative of the actual operating conditions post-quantization. This involves propagating a batch of input samples through the model sequentially, with each layer's input being the output of the preceding **quantized** layers.\n",
        "\n",
        "The quantization process must follow a strict sequence both across and within layers. Within each layer, there is a predetermined order in which the submodules must be quantized, which is dictated by the dependencies between them. This order is defined by the \"sequential groups\".\n",
        "\n",
        "The steps of the algorithm are as follows:\n",
        "- Retrieve and prepare inputs for the first layer.\n",
        "- Iterate through each layer in the model:\n",
        "  - Load the current layer for processing.\n",
        "  - Within each layer, process the sequential groups of submodules:\n",
        "    - Attach forward hooks to collect input data to each submodule.\n",
        "    - Execute a forward pass through the layer to accumulate the necessary input data for quantization.\n",
        "    - Remove the hooks after data collection.\n",
        "    - Apply GPTQ to quantize the submodule weights using the accumulated input data.\n",
        "  - Perform another forward pass through the quantized layer to generate the inputs for the next layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMyrGkHqy-xg"
      },
      "outputs": [],
      "source": [
        "def get_accumulate_input_fn(name: str, hessians: Mapping[str, Tensor], num_samples: Mapping[str, int]):\n",
        "    \"\"\"Generate a callback that updates the corresponding hessians and counts when given input\n",
        "    Args:\n",
        "        name (str): module name\n",
        "        hessians (Mapping[str, Tensor]): a dict of modules' hessians, accessible by module name\n",
        "        num_samples (Mapping[str, int]): a dict of callback call counters\n",
        "    \"\"\"\n",
        "    def tmp(_, inp, out):\n",
        "        inp = inp[0].data # ... x hidden_size\n",
        "        inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "        inp = inp.t().float() # hidden_size x inputs\n",
        "        num_samples[name] += 1\n",
        "        if hessians[name] is None:\n",
        "            hessians[name] = inp.matmul(inp.t())\n",
        "        else:\n",
        "            hessians[name] += inp.matmul(inp.t())\n",
        "    return tmp\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_gptq(model: LlamaForCausalLM, batch: Tensor, bits: int):\n",
        "    \"\"\"Iterates LLaMA layers one by one and quantizes them with GPTQ\n",
        "    Args:\n",
        "        model (LlamaForCausalLM): model to quantize\n",
        "        batch (Tensor): sample model inputs\n",
        "        bits (int): number of bits to quantize to\n",
        "    \"\"\"\n",
        "    # Collect the first layer inputs, put them on .cuda() (the same as in get_batch_nll)\n",
        "    inps, position_embeddings = get_first_layer_inputs(model, batch)\n",
        "    inps = inps.cuda()\n",
        "    position_embeddings = (position_embeddings[0].cuda(), position_embeddings[1].cuda())\n",
        "\n",
        "    # Create a buffer for layer outputs\n",
        "    outs = torch.zeros_like(inps)\n",
        "\n",
        "    # Forward pass through the layers\n",
        "    layers = model.model.layers\n",
        "    for i in trange(len(model.model.layers)):\n",
        "        layer = layers[i].cuda()\n",
        "        linear_layers = find_linear_layers(layer)\n",
        "\n",
        "        hessians = {name: None for name in linear_layers}\n",
        "        num_samples = {name: 0 for name in linear_layers}\n",
        "        handles = [\n",
        "            linear_layers[name].register_forward_hook(\n",
        "                get_accumulate_input_fn(name, hessians, num_samples)\n",
        "            ) for name in linear_layers\n",
        "        ]\n",
        "        forward_pass_layer(layer, inps, outs, position_embeddings)\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "\n",
        "        for name, linear in linear_layers.items():\n",
        "            q, scale, zero = gptq(linear.weight.data, bits, 2 * hessians[name] / num_samples[name])\n",
        "            quantized_linear = QuantizedLinear(q, scale, zero, linear.bias)\n",
        "            replace_submodule(layer, name, quantized_linear)\n",
        "\n",
        "        forward_pass_layer(layer, inps, outs, position_embeddings)\n",
        "        inps, outs = outs, inps\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6lN_q-0zFej"
      },
      "source": [
        "**Evaluating the model with GPTQ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKRuiX7Vza96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d15831d0cdd34fb0b3fbdcf0f1828a39",
            "45e2a5a174eb465293406a7eca63ad6a",
            "8b5252a0278542fd9f3fa628eac812c1",
            "0158a85595684a2084f9dfb0ebf65bc6",
            "e1213fc40c4d4ec0ab20dfaaae0eec29",
            "0501d61ae307408ca17c91b1699862f1",
            "85c121c853294a28a2fa7e0a17e735b3",
            "7753b34acf7e4fd5a59a5181c038d6a7",
            "938fe60f2f3943bb8e09263fe4599913",
            "6b4caa94323f4690a4f2f1926580477e",
            "333eb77c7bf44d989b435661a2d21f76"
          ]
        },
        "outputId": "9c4c5a59-6a0c-4b6f-8187-494638862a32"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d15831d0cdd34fb0b3fbdcf0f1828a39"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, tokenizer = get_llama_model_and_tokenizer()\n",
        "llama_gptq(model, train_batch, BITS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ARPLErza7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "06a66b0e1f884a1f8b48bc5d1a1dc9a6",
            "82ee41df46b543599782c0dad0c59bb5",
            "b590eca656bc4d63a9f6632c4db3a35e",
            "19851f201bf449deae2645b1caaee653",
            "45d65041a20844da962a6bc9d8107872",
            "40dff6511c3345bf87613729c95fbfdf",
            "ecc7c131ea114c638819e9700318afd6",
            "d395340c0bc340b0bc995092919bf5d5",
            "7a650f1bda9d455d9a8fa42ba1b3b6ab",
            "cfb5b12a5d2c4972bb6ceb8879cee3a7",
            "fdbf4b9dd4e24643b3e4ee343f24a4b1",
            "921d60a434944f559b754e491a17595e",
            "5269a7a0dcc04b99b6693d0e372152aa",
            "9f4562c8887f461dbfa6faf69403db0e",
            "845db286dc8a40fc88301b60b71a2743",
            "f98885a26f994a0ea9e12cfa686ec1ea",
            "5eb6bacedf1441e8a514dfe6f099fa17",
            "9b3717f9dc2e4ed0aa30019520c42330",
            "b542eab1bbee48b0b72a6497a9fdb962",
            "894842f1244c419b8a26ef693d08922d",
            "d48c646374304885958a8dc0f3e5d7d6",
            "f35bad960fb4406e8997bba8f55457ef",
            "7dbb815a2fe94e9ba0979a818a57ba8e",
            "a4f28081fe614fa3b1ac562284d512bb",
            "0f063a66a6e843c0bbbc4f9132d54293",
            "2539af337736400b98f8324581266090",
            "dde95b2e74e24fab92caa85c26d89c92",
            "37258d5c0c8240e5a9d4b79caa44309e",
            "0c80c0af99314f488239a10fa11d36a5",
            "c47dc7e36d994d3eaac7b44bcfdde46c",
            "145121993be64e3bb0582af32ac36b49",
            "5ce4f27c5b37491a96c72b8abceca6ae",
            "6fac5ac743ec477c887ec87eaff4da43",
            "80555c90ff454c4dba6956ebf06224df",
            "a91f0485ea19409aa752acca57e7bf0d",
            "e442a5df80d0477fb3b42d6a2760dc73",
            "e8934184f6ec49928b75ee078a792d07",
            "a18fef4363ab411cb22b14975be5707e",
            "4be36e2bdc5e44359b40dd9495be535e",
            "33b70c4a21ab47fa88e02e7fb552235f",
            "eba48666efe3439eac97637c5573b622",
            "7fe9a3b88c274bb6b8b75e555b772120",
            "53f94b287f3d4e6fab81921f8b328a3d",
            "9723a10a89b74d2891a74c71bd4c25cd",
            "353ebe6e33264b5092a828fe64498faa",
            "ffc3de81e3a5400e8a62e6de5b6cf34b",
            "a618453ba01b4055b93c8f2a72022a0a",
            "935cdae94499460ba502700f613719f1",
            "a5520404861e4b4b973eca78c7e556d7",
            "f93f207f9b3444babd6e502d51d7d7b7",
            "505cfad8a38547309313925bbc34c228",
            "18efd8aa0bf14869b76a790cbe2997c3",
            "656e50c8b7ba481492df41b82d17205b",
            "336ae6b804a544289260f7da79784ddd",
            "a8854bb691ea430699ad7341a8b2359f"
          ]
        },
        "outputId": "b09969b8-b317-4838-95eb-c0dbe9589a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06a66b0e1f884a1f8b48bc5d1a1dc9a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "921d60a434944f559b754e491a17595e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dbb815a2fe94e9ba0979a818a57ba8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80555c90ff454c4dba6956ebf06224df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "353ebe6e33264b5092a828fe64498faa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPL: 11.633154330615653\n"
          ]
        }
      ],
      "source": [
        "gptq_ppl = llama_eval(model, test_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QwMLEqUza43"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "\n",
        "assert gptq_ppl < 13\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18BHd4Nnza2T"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E14awThL68JM"
      },
      "source": [
        "# Bonus: QUIK\n",
        "\n",
        "[QUIK](https://arxiv.org/abs/2310.09259) is an extension of GPTQ. It's main feature is that, in addition to model quantization, it also quantizes activations. That way, multiplication can be performed in `int`, leading to 2x-3x improvement in inference speed over GPTQ, which dequantizes the weights and performs multiplication in `float`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JipC2pi-idyg"
      },
      "source": [
        "### Different Range, Different Scales and Zeros\n",
        "\n",
        "At basic quantization level, QUIK already has a number of differences compared to GPTQ:\n",
        " 1. QUIK quantizes the weights to **signed** integer values for better stability of matrix multiplication.\n",
        " 2. QUIK enforces `zero` to be integer, to be able to perform `int` multiplication with it as well.\n",
        " 3. QUIK changes the sign of `zero` (compare `quik_dequantize` with `dequantize`) for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEtnoIS5R8Z8"
      },
      "outputs": [],
      "source": [
        "def quik_get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produce tensors scale and zero of shape (m, 1)\n",
        "        such that 0 < x / scale + zero < max_abs\"\"\"\n",
        "    if x.shape[-1] == 0:\n",
        "        return torch.ones(x.shape[:-1], dtype=x.dtype, device=x.device), torch.zeros(x.shape[:-1], dtype=torch.int, device=x.device)\n",
        "    xmin = x.min(-1)[0]\n",
        "    xmax = x.max(-1)[0]\n",
        "\n",
        "    scale = (xmax - xmin) / max_abs / 2\n",
        "    scale[scale == 0] = 1\n",
        "    zero = torch.round((xmax + xmin) / 2 / scale) # zero is int, since we want to use it in int operations\n",
        "\n",
        "    return scale.unsqueeze(-1), zero.unsqueeze(-1).to(torch.int)\n",
        "\n",
        "\n",
        "def quik_quantize(x: Tensor, scale: Tensor, zero: Tensor, bits: int) -> Tensor:\n",
        "    \"\"\"Given a tensor x quantize it, producing tensors quantized_x of torch.int8 dtype\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "        bits (int): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        Tensor: quantized tensor in int8\n",
        "    \"\"\"\n",
        "    if x.shape[-1] == 0:\n",
        "        return torch.zeros(x.shape, dtype=torch.int8, device=x.device)\n",
        "    max_abs = 2 ** (bits - 1) - 1\n",
        "    quantized_x = torch.round(x / scale) - zero\n",
        "    quantized_x = torch.clamp(quantized_x, -max_abs, max_abs) # what are the allowed values for int8?\n",
        "    return quantized_x.to(torch.int8)\n",
        "\n",
        "\n",
        "def quik_dequantize(x: Tensor, scale: Tensor, zero: Tensor) -> Tensor:\n",
        "    \"\"\"Dequantize a tensor\n",
        "    Args:\n",
        "        quantized_x (Tensor): quantized tensor in int8\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "\n",
        "    Returns:\n",
        "        Tensor: dequantized tensor\n",
        "    \"\"\"\n",
        "    return scale * x + scale * zero\n",
        "\n",
        "\n",
        "def quik_measure_and_quantize(x: Tensor, bits: float) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    \"\"\"Determine the values interval mapping parameters and quantize a tensor\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        bits (float): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor, Tensor]: quantized tensor, scale, zero\n",
        "    \"\"\"\n",
        "    max_abs = 2 ** (bits - 1) - 1\n",
        "    scale, zero = quik_get_scale_and_zero(x, max_abs)\n",
        "    x_quantized = quik_quantize(x, scale, zero, bits)\n",
        "    return x_quantized, scale, zero\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSqZunEhR-_Y"
      },
      "source": [
        "### Weight Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Vrjo7qidyj"
      },
      "source": [
        "**Full quantization**\n",
        "\n",
        "To determine the outliers, QUIK needs additional information about layer inputs. Namely, `l_inf_norms` - max module values for each input coordinate in a minibatch. We can see how it's used to extract outliers.\n",
        "\n",
        "The rest of the function, again, is copy-paste from `gptq(...)`. Feel free to reuse your code, but don't forget to replace `gptq_block` with `quik_block`.\n",
        "\n",
        "**Task:** Paste GPTQ code into QUIK:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def quik_block(block_weight: Tensor, block_hessian_inverse: Tensor, scale: Tensor, zero: Tensor, bits: int) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"NOTE: This function is allowed to alter the block_weight as we won't need those weights anymore\n",
        "\n",
        "    Args:\n",
        "        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n",
        "        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n",
        "        scale (Tensor): row-wise quantization constants of shape (OUT, 1)\n",
        "        zero (Tensor): row-wise quantization constants of shape (OUT, 1)\n",
        "        bits (int): number of bits to quantize() to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n",
        "    \"\"\"\n",
        "    block_weight = block_weight.clone()\n",
        "    quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.int8, device=block_weight.device)\n",
        "    scaled_block_error = torch.zeros_like(block_weight)\n",
        "\n",
        "    # Iterate over the block's columns\n",
        "    for i in range(block_weight.shape[1]):\n",
        "        # Get the column and the corresponding inverse Hessian\n",
        "        column_weight = block_weight[:, [i]]\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    return quantized_block_weight, scaled_block_error"
      ],
      "metadata": {
        "id": "gFHrPYcunO3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7TGlfLgidyj"
      },
      "outputs": [],
      "source": [
        "def quik(weight: torch.Tensor, bits: int, hessian: torch.Tensor, l_inf_norms: torch.Tensor, blocksize:int=128, percdamp:float=.01, n_outliers=128):\n",
        "    dtype = weight.dtype\n",
        "    weight = weight.clone().detach()\n",
        "    weight = weight.float()\n",
        "\n",
        "    # Identify and patch always-zero input coordinates\n",
        "    dead = torch.diag(hessian) == 0\n",
        "    hessian[dead, dead] = 1\n",
        "    weight[:, dead] = 0\n",
        "\n",
        "    # Identify outliers by decreasing l_inf_norms. Sort the remained by decrasing hessian values\n",
        "    perm = torch.argsort(l_inf_norms, descending=True)\n",
        "    perm[n_outliers:] = perm[n_outliers:][torch.argsort(torch.diag(hessian)[perm][n_outliers:], descending=True)]\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    weight = ...\n",
        "    hessian = ...\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    # Process outliers\n",
        "    outlier_weight = weight[:,:n_outliers]\n",
        "    weight = weight[:,n_outliers:]\n",
        "    num_columns = weight.shape[1]\n",
        "    hessian = hessian[n_outliers:,:][:,n_outliers:]\n",
        "\n",
        "    max_abs = 2 ** (bits - 1) - 1\n",
        "    scale, zero = quik_get_scale_and_zero(weight, max_abs)\n",
        "\n",
        "    # Process the Hessian to obtain the precomputed inverse Hessian\n",
        "    damp = percdamp * torch.mean(torch.diag(hessian))\n",
        "    diag = torch.arange(num_columns, device=weight.device)\n",
        "    hessian[diag, diag] += damp\n",
        "    hessian = torch.linalg.cholesky(hessian)\n",
        "    hessian = torch.cholesky_inverse(hessian)\n",
        "    hessian = torch.linalg.cholesky(hessian, upper=True)\n",
        "    hessian_inverse = hessian\n",
        "\n",
        "    # Iterate over the columns in blocks\n",
        "    quantized_weight = torch.zeros(weight.shape, dtype=torch.int8, device=weight.device)\n",
        "    for block_start in range(0, num_columns, blocksize):\n",
        "        block_end = min(block_start + blocksize, num_columns)\n",
        "\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        # Get the next block and quantize it\n",
        "        quantized_weight[:, block_start: block_end], ...\n",
        "        # Tune all the following blocks to mitigate the quantization error\n",
        "        weight[:, block_end: ] -= ...\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    return quantized_weight, scale.to(dtype), zero, outlier_weight.to(dtype), perm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GAoLdmOSF6-"
      },
      "source": [
        "### QUIK Linear Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RIy26cHidym"
      },
      "source": [
        "Reminder: `output = custom_kernel.int8_matmul(X, Y)` and `output = custom_kernel.int4_matmul(X, Y)` compute $XY^T$ (same as `nn.functional.linear`).\n",
        "\n",
        "Notice that `int8_matmul` takes the normal `torch.int8` tensors, but int4_matmul expects `int4` values densely packed into `torch.uint8` tensors. This is exactly what we wrote **Dense Integer Packing** for. They have a <font color='red'>severe limitation</font>: the dimension of multiplication must be divisible by 16.\n",
        "\n",
        "Now you have to implement the quantized forward pass:\n",
        "\n",
        " $$\n",
        " \\begin{align}\n",
        "    XW^T &= (Q_x \\cdot scale_x + zero_x \\cdot scale_x)(Q_w \\cdot scale_w + zero_w \\cdot scale_w)^T =\\\\\n",
        "    &= (Q_x  + zero_x)(Q_w + zero_w)^T \\cdot (scale_x \\odot scale_w^T) =\\\\\n",
        "    &= (Q_xQ_w^T + Q_x zero_w^T + zero_x Q_w^T + zero_x zero_w^T) \\cdot (scale_x \\odot scale_w^T)\n",
        " \\end{align}\n",
        " $$\n",
        "\n",
        "because of the kernel limitations mentioned above, only the largest integer multiplication ($Q_xQ_w^T$) can be done in `int`. Perform the other ones in `float`.\n",
        "\n",
        "**Task (3pt)**: implement QUIK forward pass and measure it's performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG8Y2aP4wDMK"
      },
      "outputs": [],
      "source": [
        "class QuikLinear(nn.Module):\n",
        "    def __init__(self, quantized_weight, weight_scale, weight_zero, outlier_weight, bias, bits: int, perm):\n",
        "        super().__init__()\n",
        "        self.bits = bits\n",
        "        self.perm = perm\n",
        "        self.n_outliers = outlier_weight.shape[1]\n",
        "\n",
        "        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n",
        "        self.weight_scale = nn.Parameter(weight_scale, requires_grad=False)\n",
        "        self.weight_zero = nn.Parameter(weight_zero, requires_grad=False)\n",
        "\n",
        "        self.outlier_weight = nn.Parameter(outlier_weight, requires_grad=False)\n",
        "        self.weights_reduced = self.quantized_weight.to(torch.int32).sum(dim=1).float()\n",
        "\n",
        "        if bias is not None:\n",
        "            self.bias = nn.Parameter(bias.data.clone().detach())\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        out_size, in_size = self.quantized_weight.shape\n",
        "        input = input[...,self.perm]\n",
        "        input_quantized, input_scale, input_zero = quik_measure_and_quantize(input[...,self.n_outliers:], self.bits)\n",
        "\n",
        "        outliers_result = F.linear(input[...,:self.n_outliers], self.outlier_weight, self.bias)\n",
        "        if input_quantized.shape[-1] == 0:\n",
        "            return outliers_result\n",
        "\n",
        "        # Convert necessary components to float\n",
        "        inputs_reduced = input_quantized.to(torch.int32).sum(dim=-1).float()\n",
        "        input_zero = input_zero.float()\n",
        "        weight_zero = self.weight_zero.data.float()\n",
        "\n",
        "        # Fully int operations\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        quantized_result = ...\n",
        "        quantized_result = quantized_result.float()\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "        # I wish those were int, but float operations\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        quantized_result +=                   # inputs_reduced and weight_zero\n",
        "        quantized_result +=                   # input_zero and weights_reduced\n",
        "        quantized_result +=                   # input_zero and weight_zero\n",
        "        quantized_result = quantized_result * # something with scales\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "        quantized_result = quantized_result.to(torch.float16)\n",
        "        results = quantized_result + outliers_result\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcT93zaCNA4r"
      },
      "source": [
        "### Loading the First LLaMA Layer Attention Q Projection and its Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CU8tqdd0z0h"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = get_llama_model_and_tokenizer()\n",
        "inps, _ = get_first_layer_inputs(model, train_batch)\n",
        "inps = inps.cuda()\n",
        "layer = model.model.layers[0].self_attn.q_proj.cuda()\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n3SauNWNA4r"
      },
      "source": [
        "### Gathering the Layer Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hsx8WPe1Kek"
      },
      "outputs": [],
      "source": [
        "class AccumulatedInput:\n",
        "    hessians = None\n",
        "    num_samples = 0\n",
        "    actnorms = None\n",
        "\n",
        "@torch.no_grad()\n",
        "def accumulate_layer_input(_, inp, out):\n",
        "    inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "    inp = inp.t().float() # hidden_size x inputs\n",
        "    AccumulatedInput.num_samples += 1\n",
        "    if AccumulatedInput.hessians is None:\n",
        "        AccumulatedInput.hessians = inp.matmul(inp.t())\n",
        "        AccumulatedInput.actnorms = inp.abs().amax(dim=1)\n",
        "    else:\n",
        "        AccumulatedInput.hessians += inp.matmul(inp.t())\n",
        "        AccumulatedInput.actnorms = torch.maximum(AccumulatedInput.actnorms, inp.abs().amax(dim=1))\n",
        "\n",
        "\n",
        "for inp in inps:\n",
        "    accumulate_layer_input(None, inp, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frvUz5b51Of3"
      },
      "outputs": [],
      "source": [
        "OUT = layer.weight.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWC3HT2D1Pu0"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reference = torch.stack(tuple(layer(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeYCLNoNA4s"
      },
      "source": [
        "### Benchmarking MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI6ShN5gZby9",
        "outputId": "81698e57-0462-4966-e3c3-03b3ce973160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 7.314414688153192e-05\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero = measure_and_quantize(layer.weight.data, 8)\n",
        "    shit = QuantizedLinear(quantized_weight, scale, zero, layer.bias)\n",
        "\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "\n",
        "    rtn_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n",
        "    print(\"MSE:\", rtn_mse)\n",
        "\n",
        "assert rtn_mse < 8e-5 and rtn_mse > 7e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AifKxdLHZhii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f09ae80-753e-476c-dfb0-44b20f05156e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1.862943645392079e-05\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero = gptq(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples)\n",
        "    shit = QuantizedLinear(quantized_weight, scale, zero, layer.bias)\n",
        "\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "\n",
        "    gptq_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n",
        "    print(\"MSE:\", gptq_mse)\n",
        "\n",
        "assert gptq_mse < 3e-5 and gptq_mse > 1.5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBAhlxPcF0Mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf442e6-e0d2-4665-dec1-b9889c8d0294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 3.1697094527771696e-05\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero, outlier_weight, perm = quik(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples, AccumulatedInput.actnorms, n_outliers=0)\n",
        "    shit = QuikLinear(quantized_weight, scale, zero, outlier_weight, layer.bias, 8, perm)\n",
        "\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "\n",
        "    quik_0_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n",
        "    print(\"MSE:\", quik_0_mse)\n",
        "\n",
        "assert quik_0_mse < 3.2e-5 and quik_0_mse > 2.8e-5, f\"{quik_0_mse}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euq4H-VwZj9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28ee6e8-369b-40a7-d3ec-19a06f0cfc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 2.591484008007683e-05\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero, outlier_weight, perm = quik(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples, AccumulatedInput.actnorms, n_outliers=256)\n",
        "    shit = QuikLinear(quantized_weight, scale, zero, outlier_weight, layer.bias, 8, perm)\n",
        "\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "\n",
        "    quik_256_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n",
        "    print(\"MSE:\", quik_256_mse)\n",
        "\n",
        "assert quik_256_mse < 2.8e-5 and quik_256_mse > 2.5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-J0ychH4GDUk",
        "outputId": "1fca3ed3-d25f-4a09-c483-70dea00cc307"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fruits = ['GPTQ', 'QUIK\\n256 outliers', 'QUIK\\n0 outliers', 'RNT']\n",
        "counts = [gptq_mse, quik_256_mse, quik_0_mse, rtn_mse]\n",
        "bar_labels = ['green', 'blue', 'blue', 'red']\n",
        "bar_colors = ['green', 'blue', 'blue', 'red']\n",
        "\n",
        "plt.bar(fruits, counts, label=bar_labels, color=bar_colors)\n",
        "plt.ylabel(\"Layer MSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF78dQBVIASz"
      },
      "source": [
        "As we can see, QUIK allows for errors smaller than RTN despite utilizing quantized activations, and addition of outliesr further decreases the error, bringing it closer to GPTQ. As we have seen in `benchmark.ipynb`, `int` matmul can lead to 2x-3x speedup over `float16`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFp2mztPGYY4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "JipC2pi-idyg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "-1.-1.-1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e2aa6f23f38e44dd8bd3b03f4cbdf7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3e518de58d84095949ad5dab0a8e345",
              "IPY_MODEL_26143660de5447ab8be249ef634dd96d",
              "IPY_MODEL_c66e193de38d4028bbe8a095dd630f22"
            ],
            "layout": "IPY_MODEL_81942199469a4467a1a5388c66b5d0e7"
          }
        },
        "a3e518de58d84095949ad5dab0a8e345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c6cc9f0a4148aa8be74d94f2ce6920",
            "placeholder": "​",
            "style": "IPY_MODEL_51d2b87182704b57ae4462a073f4e323",
            "value": "Iterating BATCH_SIZE: 100%"
          }
        },
        "26143660de5447ab8be249ef634dd96d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c10c01ed6fd4a268702f22d8e0bb2c7",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e34ce1fc4ab541769042b489a07c1d44",
            "value": 10
          }
        },
        "c66e193de38d4028bbe8a095dd630f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_464a5860e668481fbaab8da5f56abb80",
            "placeholder": "​",
            "style": "IPY_MODEL_fc26123cd44d4ddf8095175b1477e3ee",
            "value": " 10/10 [00:15&lt;00:00,  1.56s/it]"
          }
        },
        "81942199469a4467a1a5388c66b5d0e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c6cc9f0a4148aa8be74d94f2ce6920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51d2b87182704b57ae4462a073f4e323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c10c01ed6fd4a268702f22d8e0bb2c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e34ce1fc4ab541769042b489a07c1d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "464a5860e668481fbaab8da5f56abb80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc26123cd44d4ddf8095175b1477e3ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}